import onnxruntime as ort
import numpy as np
import time

# ----- Cáº¥u hÃ¬nh -----
onnx_path = "/media/icnlab/Data/Thang/plan_dieases/vit_xai/src/export/mobilevitxxs.onnx"
num_images = 100
img_size = (3, 224, 224)

# ----- Táº¡o session chá»‰ cháº¡y CPU -----
sess_options = ort.SessionOptions()
sess_options.intra_op_num_threads = 4
sess_options.inter_op_num_threads = 1
sess_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL

session = ort.InferenceSession(
    onnx_path,
    sess_options,
    providers=["CPUExecutionProvider"],
)

print("Provider Ä‘ang dÃ¹ng:", session.get_providers())

# ----- Láº¥y tÃªn input/output -----
input_name = session.get_inputs()[0].name
output_name = session.get_outputs()[0].name

# ----- Táº¡o batch input (100 áº£nh) -----
batch_input = np.random.randn(num_images, *img_size).astype(np.float32)

# ----- Warm-up Ä‘áº§y Ä‘á»§ (10 láº§n) -----
print("\nğŸ”¥ Warm-up...")
for _ in range(10):
    _ = session.run([output_name], {input_name: batch_input[:1]})


# ----- Äo BATCH (so sÃ¡nh) -----
print("\nğŸ” Äo batch processing (100 áº£nh cÃ¹ng lÃºc)...")
start = time.perf_counter()
outputs = session.run([output_name], {input_name: batch_input})
end = time.perf_counter()

batch_time = (end - start) * 1000
batch_avg = batch_time / num_images

print(f"  â€¢ Batch total: {batch_time:.2f} ms")
print(f"  â€¢ Batch avg: {batch_avg:.2f} ms/áº£nh")
print(f"  â€¢ Output shape: {outputs[0].shape}")

# ----- Äo BATCH=1 (latency) -----
print("\nâ±ï¸  Äo latency (batch=1)...")
start = time.perf_counter()
for _ in range(num_images):
    _ = session.run([output_name], {input_name: batch_input[:1]})
end = time.perf_counter()

latency_time = (end - start) * 1000
latency_avg = latency_time / num_images

print(f"  â€¢ Total: {latency_time:.2f} ms cho {num_images} láº§n")
print(f"  â€¢ Avg latency: {latency_avg:.2f} ms/áº£nh")
